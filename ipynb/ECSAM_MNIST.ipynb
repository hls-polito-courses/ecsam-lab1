{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYFcv7STwwvK"
   },
   "source": [
    "# MNIST Competition\n",
    "Let's see how some networks perform with the MNIST handwritten digits dataset.\n",
    "Run the following cell in order to setup the environment and load all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eNR7eKywrW4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.system(\"mkdir -p content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGpdt8iJrTrQ"
   },
   "source": [
    "## The MNIST dataset\n",
    "Let's import the MNIST dataset and convert the class vectors (i.e. digits) to one-hot binary classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmovVONq0BTU",
    "outputId": "e36cfa48-bcdd-4735-e2e8-5687ca92ae44"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "#x_train = np.expand_dims(x_train, -1)\n",
    "#x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erCDnWN1xG5S"
   },
   "source": [
    "## 1. `LeNet`\n",
    "The following code defines a CNN very similar to the classic LeNet developed by Yann LeCun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdrTSH75xTbO"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "class LeNet:\n",
    "  @staticmethod\n",
    "  def build():\n",
    "    # initialize the model along with the input shape to be\n",
    "    # \"channels last\" and the channels dimension itself\n",
    "    model = Sequential()\n",
    "    inputShape = (784,1)\n",
    "    model.add(Input(shape=(784,)))\n",
    "    model.add(Reshape((28, 28, 1), input_shape=(784,)))\n",
    "    model.add(Conv2D(filters=6, kernel_size=(3, 3)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(AveragePooling2D())\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3, 3)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(AveragePooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=120))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(units=84))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(units=10, activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywqB9Wvarohq"
   },
   "source": [
    "Here we define the optimizer, build the model, and report the summary.\n",
    "Moreover, you can run this cell if you want to restart your training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfR1VVEA0d9p",
    "outputId": "d30eaa79-f6dd-4b83-f008-a5b1aa26caa8"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "init_lr = 1e-3\n",
    "opt = Adam(lr=init_lr, decay=init_lr / (epochs * 0.5))\n",
    "#checkpoint_filepath = '/content/checkpoint_LeNet_{epoch:02d}_acc_{acc:.4f}_loss_{loss:.4f}_valacc_{val_acc:.4f}_valloss_{val_loss:.4f}.hdf5'\n",
    "checkpoint_filepath = './content/checkpoint_LeNet.hdf5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_acc', #val_accuracy in tf 2.0\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    save_best_only=False)\n",
    "\n",
    "model = LeNet.build()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"acc\"])\n",
    "model.summary()\n",
    "\n",
    "#init_op = tf.global_variables_initializer()\n",
    "\n",
    "#sess = tf.Session()\n",
    "#sess.run(init_op)\n",
    "\n",
    "# To clean the history of the training.\n",
    "H = []\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "acc_list = []\n",
    "val_acc_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6-iuciesQVS"
   },
   "source": [
    "Time to do the training and report the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOnbyEpk0red",
    "outputId": "fb8c2d08-bf1f-46e6-b248-a88656b47eb2"
   },
   "outputs": [],
   "source": [
    "H = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[model_checkpoint_callback], validation_split=0.1)\n",
    "\n",
    "loss_list += H.history['loss']\n",
    "val_loss_list += H.history['val_loss']\n",
    "acc_list += H.history['acc']\n",
    "val_acc_list += H.history['val_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6liPg_kNaYkh"
   },
   "source": [
    "Since after training the \"model\" variable contains the model trained with the weights found on the last epoch, we load the best weights saved with the callback.\n",
    "\n",
    "Then we evaluate the best found solution on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4D2i91GakHG",
    "outputId": "9c1ffdf6-369c-4224-fda8-d2291ac8e8de"
   },
   "outputs": [],
   "source": [
    "# this is the score of the last epoch\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# reload best weights\n",
    "model.load_weights(checkpoint_filepath)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7hN4lcusdWp"
   },
   "source": [
    "Some useful plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "yCEa15Ip8fMQ",
    "outputId": "064c57c1-ca13-4abf-b4b6-7b80777765d3"
   },
   "outputs": [],
   "source": [
    "loss_list_plot = loss_list\n",
    "val_loss_list_plot = val_loss_list\n",
    "acc_list_plot = acc_list\n",
    "val_acc_list_plot = val_acc_list\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "N = np.arange(0, len(loss_list_plot))\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, loss_list_plot, label=\"train_loss\")\n",
    "plt.plot(N, val_loss_list_plot, label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.ylim(0, 0.10) # edit this line if needed to show a better graph\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(N, acc_list_plot, label=\"train_acc\")\n",
    "plt.plot(N, val_acc_list_plot, label=\"val_acc\")\n",
    "plt.title(\"Training Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.ylim(0.95, 1) # edit this line if needed to show a better graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZ1EvQB7sgjG"
   },
   "source": [
    "## 2. `LeNet2` with Antirectifier\n",
    "Here we replace the first two ReLU activations with a new activation called \"the Antirectifier\". We also increase the number of filters for the second convolution layer and the size of the last two FC layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDI9fBeGZAwO"
   },
   "outputs": [],
   "source": [
    "class Antirectifier(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Antirectifier, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        output_dim = input_shape[-1]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs -= tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        pos = tf.nn.relu(inputs)\n",
    "        neg = tf.nn.relu(-inputs)\n",
    "        concatenated = tf.concat([pos, neg], axis=-1)\n",
    "        return concatenated\n",
    "\n",
    "    def get_config(self):\n",
    "        # Implement get_config to enable serialization. This is optional.\n",
    "        base_config = super(Antirectifier, self).get_config()\n",
    "        return dict(list(base_config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fro6xaiWIi1f"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "class LeNet2:\n",
    "  @staticmethod\n",
    "  def build():\n",
    "    # initialize the model along with the input shape to be\n",
    "    # \"channels last\" and the channels dimension itself\n",
    "    model = Sequential()\n",
    "    inputShape = (784,1)\n",
    "    model.add(Input(shape=(784,)))\n",
    "    model.add(Reshape((28, 28, 1), input_shape=(784,)))\n",
    "    model.add(Conv2D(filters=6, kernel_size=(3, 3)))\n",
    "    model.add(Antirectifier())\n",
    "    model.add(AveragePooling2D())\n",
    "    #model.add(Conv2D(filters=16, kernel_size=(3, 3)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3)))\n",
    "    model.add(Antirectifier())\n",
    "    model.add(AveragePooling2D())\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(units=120))\n",
    "    model.add(Dense(units=128))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dense(units=84))\n",
    "    model.add(Dense(units=128))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(units=10, activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8XNNVyfY8Mt",
    "outputId": "39d9f98f-f2cf-4001-d05e-7379b056af61"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "init_lr = 1e-3\n",
    "opt = Adam(lr=init_lr, decay=init_lr / (epochs * 0.5))\n",
    "#checkpoint_filepath2 = '/content/checkpoint_LeNet2_{epoch:02d}_acc_{acc:.4f}_loss_{loss:.4f}_valacc_{val_acc:.4f}_valloss_{val_loss:.4f}.hdf5'\n",
    "checkpoint_filepath2 = './content/checkpoint_LeNet2.hdf5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath2,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_acc', #val_accuracy in tf 2.0\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    save_best_only=False)\n",
    "\n",
    "model2 = LeNet2.build()\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"acc\"])\n",
    "model2.summary()\n",
    "\n",
    "#init_op = tf.global_variables_initializer()\n",
    "\n",
    "#sess = tf.Session()\n",
    "#sess.run(init_op)\n",
    "\n",
    "# To clean the history of the training.\n",
    "H2 = []\n",
    "loss_list2 = []\n",
    "val_loss_list2 = []\n",
    "acc_list2 = []\n",
    "val_acc_list2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyGqpprgstrX"
   },
   "source": [
    "Let's train the new version of LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpMr-8PbJO0Z",
    "outputId": "c83c84ed-e003-4cff-975c-f042594712d3"
   },
   "outputs": [],
   "source": [
    "H2 = model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[model_checkpoint_callback], validation_split=0.1)\n",
    "\n",
    "loss_list2 += H2.history['loss']\n",
    "val_loss_list2 += H2.history['val_loss']\n",
    "acc_list2 += H2.history['acc']\n",
    "val_acc_list2 += H2.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tr7bWKXRZFD1",
    "outputId": "c6bb9e81-d119-4bfe-9fcf-6a300717cf6c"
   },
   "outputs": [],
   "source": [
    "# this is the score of the last epoch\n",
    "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score2[0])\n",
    "print(\"Test accuracy:\", score2[1])\n",
    "\n",
    "# reload best weights\n",
    "model2.load_weights(checkpoint_filepath2)\n",
    "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score2[0])\n",
    "print(\"Test accuracy:\", score2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "KDTSyaQdaamM",
    "outputId": "63a954d0-8493-488c-93f1-7d22c8eaa858"
   },
   "outputs": [],
   "source": [
    "loss_list_plot = loss_list2\n",
    "val_loss_list_plot = val_loss_list2\n",
    "acc_list_plot = acc_list2\n",
    "val_acc_list_plot = val_acc_list2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "N = np.arange(0, len(loss_list_plot))\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, loss_list_plot, label=\"train_loss\")\n",
    "plt.plot(N, val_loss_list_plot, label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.ylim(0, 0.10) # edit this line if needed to show a better graph\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(N, acc_list_plot, label=\"train_acc\")\n",
    "plt.plot(N, val_acc_list_plot, label=\"val_acc\")\n",
    "plt.title(\"Training Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.ylim(0.95, 1) # edit this line if needed to show a better graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWyGxc6as3rr"
   },
   "source": [
    "## 3. `MLP`\n",
    "As the title says, we now try with a multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3I0SrDcMcry"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "class MLP:\n",
    "  @staticmethod\n",
    "  def build():\n",
    "    # initialize the model along with the input shape to be\n",
    "    # \"channels last\" and the channels dimension itself\n",
    "    model = Sequential()\n",
    "    inputShape = (784,)\n",
    "    model.add(Input(shape=(784,)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lX5W-QU3bLyu",
    "outputId": "dc08ac14-c0dc-45cf-cff1-85b256ed0d71"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "init_lr = 1e-3\n",
    "opt = Adam(lr=init_lr, decay=init_lr / (epochs * 0.5))\n",
    "#checkpoint_filepath3 = '/content/checkpoint_MLP_{epoch:02d}_acc_{acc:.4f}_loss_{loss:.4f}_valacc_{val_acc:.4f}_valloss_{val_loss:.4f}.hdf5'\n",
    "checkpoint_filepath3 = './content/checkpoint_MLP.hdf5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath3,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_acc', #val_accuracy in tf 2.0\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    save_best_only=False)\n",
    "\n",
    "model3 = MLP.build()\n",
    "model3.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"acc\"])\n",
    "model3.summary()\n",
    "\n",
    "#init_op = tf.global_variables_initializer()\n",
    "\n",
    "#sess = tf.Session()\n",
    "#sess.run(init_op)\n",
    "\n",
    "# To clean the history of the training.\n",
    "H3 = []\n",
    "loss_list3 = []\n",
    "val_loss_list3 = []\n",
    "acc_list3 = []\n",
    "val_acc_list3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHFOQBYDM9xX",
    "outputId": "76e9a6a5-3532-49b1-c10c-1d8a5ce76092"
   },
   "outputs": [],
   "source": [
    "H3 = model3.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[model_checkpoint_callback], validation_split=0.1)\n",
    "\n",
    "loss_list3 += H3.history['loss']\n",
    "val_loss_list3 += H3.history['val_loss']\n",
    "acc_list3 += H3.history['acc']\n",
    "val_acc_list3 += H3.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbJN9MpEbnMb",
    "outputId": "6cce0d6f-202a-442c-b800-ca57e7a05abc"
   },
   "outputs": [],
   "source": [
    "# this is the score of the last epoch\n",
    "score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score3[0])\n",
    "print(\"Test accuracy:\", score3[1])\n",
    "\n",
    "# reload best weights\n",
    "model3.load_weights(checkpoint_filepath3)\n",
    "score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score3[0])\n",
    "print(\"Test accuracy:\", score3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "gpCxLpFvOASb",
    "outputId": "0ad63501-cae9-4213-d54c-9dba870ef634"
   },
   "outputs": [],
   "source": [
    "loss_list_plot = loss_list3\n",
    "val_loss_list_plot = val_loss_list3\n",
    "acc_list_plot = acc_list3\n",
    "val_acc_list_plot = val_acc_list3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "N = np.arange(0, len(loss_list_plot))\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, loss_list_plot, label=\"train_loss\")\n",
    "plt.plot(N, val_loss_list_plot, label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.ylim(0, 0.10) # edit this line if needed to show a better graph\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(N, acc_list_plot, label=\"accuracy\")\n",
    "plt.plot(N, val_acc_list_plot, label=\"val_accuracy\")\n",
    "plt.title(\"Training Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.ylim(0.95, 1) # edit this line if needed to show a better graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVl5RClgtJ-w"
   },
   "source": [
    "## 4. The `Mini CNN`\n",
    "Finally, we use a small CNN with very few parameters and an Antirectifier as activation function. Let's see if it can compete with the previous networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7S1Imm5aZMp"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "class MINICNN:\n",
    "  @staticmethod\n",
    "  def build():\n",
    "    # initialize the model along with the input shape to be\n",
    "    # \"channels last\" and the channels dimension itself\n",
    "    model = Sequential()\n",
    "    inputShape = (784,1)\n",
    "    model.add(Input(shape=(784,)))\n",
    "    model.add(Reshape((28, 28, 1), input_shape=(784,)))\n",
    "    model.add(Conv2D(16, kernel_size=(3, 3)))\n",
    "    model.add(Antirectifier())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3)))\n",
    "    model.add(Antirectifier())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaF3NAgUcRIl",
    "outputId": "6ce6ec27-6dc2-46d6-b63a-4ce030ec2dfc"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "init_lr = 1e-3\n",
    "opt = Adam(lr=init_lr, decay=init_lr / (epochs * 0.5))\n",
    "#checkpoint_filepath4 = '/content/checkpoint_MINICNN_{epoch:02d}_acc_{acc:.4f}_loss_{loss:.4f}_valacc_{val_acc:.4f}_valloss_{val_loss:.4f}.hdf5'\n",
    "checkpoint_filepath4 = './content/checkpoint_MINICNN.hdf5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath4,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_acc', #val_accuracy in tf 2.0\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    save_best_only=False)\n",
    "\n",
    "model4 = MINICNN.build()\n",
    "model4.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"acc\"])\n",
    "model4.summary()\n",
    "\n",
    "#init_op = tf.global_variables_initializer()\n",
    "\n",
    "#sess = tf.Session()\n",
    "#sess.run(init_op)\n",
    "\n",
    "# To clean the history of the training.\n",
    "H4 = []\n",
    "loss_list4 = []\n",
    "val_loss_list4 = []\n",
    "acc_list4 = []\n",
    "val_acc_list4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J76HpNwbcks_",
    "outputId": "b04ddddb-649e-4bbb-dfad-8b35dcc0c1c4"
   },
   "outputs": [],
   "source": [
    "H4 = model4.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[model_checkpoint_callback], validation_split=0.1)\n",
    "\n",
    "loss_list4 += H4.history['loss']\n",
    "val_loss_list4 += H4.history['val_loss']\n",
    "acc_list4 += H4.history['acc']\n",
    "val_acc_list4 += H4.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEj1hCZzcTHI",
    "outputId": "3541a748-3632-4daf-96eb-fd9db0e784b4"
   },
   "outputs": [],
   "source": [
    "# this is the score of the last epoch\n",
    "score4 = model4.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score4[0])\n",
    "print(\"Test accuracy:\", score4[1])\n",
    "\n",
    "# reload best weights\n",
    "model4.load_weights(checkpoint_filepath4)\n",
    "score4 = model4.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score4[0])\n",
    "print(\"Test accuracy:\", score4[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "18krpVdNczrr",
    "outputId": "d4cddd59-7540-42a6-fbb2-20fc3fcee445"
   },
   "outputs": [],
   "source": [
    "loss_list_plot = loss_list4\n",
    "val_loss_list_plot = val_loss_list4\n",
    "acc_list_plot = acc_list4\n",
    "val_acc_list_plot = val_acc_list4\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "N = np.arange(0, len(loss_list_plot))\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, loss_list_plot, label=\"train_loss\")\n",
    "plt.plot(N, val_loss_list_plot, label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.ylim(0, 0.10) # edit this line if needed to show a better graph\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(N, acc_list_plot, label=\"train_acc\")\n",
    "plt.plot(N, val_acc_list_plot, label=\"val_acc\")\n",
    "plt.title(\"Training Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.ylim(0.95, 1) # edit this line if needed to show a better graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9M8GvorQCIJ"
   },
   "source": [
    "# Export the final solution\n",
    "After selecting your final implementation of the network for MNIST classification, here you can export its frozen graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl8dzol1P8vc"
   },
   "source": [
    "## Export `LeNet2`\n",
    "Load the model checkpoint and freeze the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "CrVCx8AJBJEh",
    "outputId": "e0bacc2b-0688-448f-cb32-9fd40b87e04f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                print(node)\n",
    "                node.device = \"\"\n",
    "        print(output_names)\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# This line must be executed before loading Keras model.\n",
    "# If you use dropout, batch normalization or any other layers like these\n",
    "# (which have not trainable but calculating values),\n",
    "# you should change the learning phase of keras backend\n",
    "K.set_learning_phase(0) # 0 testing, 1 training mode\n",
    "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model4.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wc3BDbv8t88"
   },
   "outputs": [],
   "source": [
    "tf.train.write_graph(frozen_graph, \"model\", \"tf_model_MNIST.pb\", as_text=False)\n",
    "tf.train.write_graph(frozen_graph, \"model\", \"tf_model_MNIST.pbtxt\", as_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsd-0nUtUR8X"
   },
   "source": [
    "## Edit `LeNet2` graph\n",
    "On the left of this notebook, click on the files icon and browse the `ipynb/model` folder. It should now contain two files, `tf_model_MNIST.pb` and `tf_model_MNIST.pbtxt`. We need to edit them for the next steps.\n",
    "\n",
    "Open the `.pbtxt` file with and look for the first occurrence of the \"antirectifier\". You will find a part of the frozen graph that corresponds to the various operations of the antirectifier layer:\n",
    "```\n",
    "node {\n",
    "  name: \"antirectifier/Mean/reduction_indices\"\n",
    "  op: \"Const\"\n",
    "  attr {\n",
    "    key: \"dtype\"\n",
    "    value {\n",
    "      type: DT_INT32\n",
    "    }\n",
    "  }\n",
    "  attr {\n",
    "    key: \"value\"\n",
    "    value {\n",
    "      tensor {\n",
    "        dtype: DT_INT32\n",
    "        tensor_shape {\n",
    "        }\n",
    "        int_val: -1\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "node {\n",
    "  name: \"antirectifier/Mean\"\n",
    "  op: \"Mean\"\n",
    "  input: \"conv2d/BiasAdd\"\n",
    "  input: \"antirectifier/Mean/reduction_indices\"\n",
    "  attr {\n",
    "    key: \"T\"\n",
    "    value {\n",
    "      type: DT_FLOAT\n",
    "    }\n",
    "  }\n",
    "  attr {\n",
    "    key: \"Tidx\"\n",
    "    value {\n",
    "      type: DT_INT32\n",
    "    }\n",
    "  }\n",
    "  attr {\n",
    "    key: \"keep_dims\"\n",
    "    value {\n",
    "      b: true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "node {\n",
    "  name: \"antirectifier/sub\"\n",
    "  op: \"Sub\"\n",
    "  input: \"conv2d/BiasAdd\"\n",
    "  input: \"antirectifier/Mean\"\n",
    "  attr {\n",
    "    key: \"T\"\n",
    "    value {\n",
    "      type: DT_FLOAT\n",
    "    }\n",
    "  }\n",
    "}\n",
    "node {\n",
    "  name: \"antirectifier/Relu\"\n",
    "  op: \"Relu\"\n",
    "  input: \"antirectifier/sub\"\n",
    "  attr {\n",
    "    key: \"T\"\n",
    "    value {\n",
    "      type: DT_FLOAT\n",
    "    }\n",
    "  }\n",
    "}\n",
    "node {\n",
    "  name: \"antirectifier/Neg\"\n",
    "  op: \"Neg\"\n",
    "  input: \"antirectifier/sub\"\n",
    "  attr {\n",
    "    key: \"T\"\n",
    "    value {\n",
    "      type: DT_FLOAT\n",
    "    }\n",
    "  }\n",
    "}\n",
    "node {\n",
    "  name: \"antirectifier/Relu_1\"\n",
    "  op: \"Relu\"\n",
    "  input: \"antirectifier/Neg\"\n",
    "  attr {\n",
    "    key: \"T\"\n",
    "    value {\n",
    "      type: DT_FLOAT\n",
    "    }\n",
    "  }\n",
    "}\n",
    "node {\n",
    "  name: \"antirectifier/concat/axis\"\n",
    "  op: \"Const\"\n",
    "  attr {\n",
    "    key: \"dtype\"\n",
    "    value {\n",
    "      type: DT_INT32\n",
    "    }\n",
    "  }\n",
    "  attr {\n",
    "    key: \"value\"\n",
    "    value {\n",
    "      tensor {\n",
    "        dtype: DT_INT32\n",
    "        tensor_shape {\n",
    "        }\n",
    "        int_val: -1\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "node {\n",
    "  name: \"antirectifier/concat\"\n",
    "  op: \"ConcatV2\"\n",
    "  input: \"antirectifier/Relu\"\n",
    "  input: \"antirectifier/Relu_1\"\n",
    "  input: \"antirectifier/concat/axis\"\n",
    "  attr {\n",
    "    key: \"N\"\n",
    "    value {\n",
    "      i: 2\n",
    "    }\n",
    "  }\n",
    "  attr {\n",
    "    key: \"T\"\n",
    "    value {\n",
    "      type: DT_FLOAT\n",
    "    }\n",
    "  }\n",
    "  attr {\n",
    "    key: \"Tidx\"\n",
    "    value {\n",
    "      type: DT_INT32\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "The goal now is to pass the frozen graph to OpenVINO's model optimizer. However, we don't want that all those operations are mapped to their equivalent OpenVINO operation: we want instead to have our own version of the Antirectifier described with a new OpenCL code. Therefore, we need to replace the entire block above with a simple node as follows:\n",
    "\n",
    "```\n",
    "node {\n",
    "  name: \"antirectifier\"\n",
    "  op: \"Antirectifier\"\n",
    "  input: \"conv2d/BiasAdd\"\n",
    "  attr {\n",
    "    key: \"T\"\n",
    "    value {\n",
    "      type: DT_FLOAT\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "In this case, the input to the antirectifier block comes from ``\"conv2d/BiasAdd\"`` in the frozen graph. In your case, you should look for which node is the input and use that node instead of what we used here. Another change is related to the node that receives the output of the Antirectifier as an input. In this case, the following node\n",
    "```\n",
    "node {\n",
    "  name: \"average_pooling2d/AvgPool\"\n",
    "  op: \"AvgPool\"\n",
    "  input: \"antirectifier/concat\"\n",
    "  ...\n",
    "```\n",
    "must be replaced as follows\n",
    "```\n",
    "node {\n",
    "  name: \"average_pooling2d/AvgPool\"\n",
    "  op: \"AvgPool\"\n",
    "  input: \"antirectifier\"\n",
    "  ...\n",
    "```\n",
    "because we would have replaced all the internal nodes of the antirectifier with a single node.\n",
    "In this network (and possibly in your network too) there are different occurences of the antirectifier. Each block of nodes must be replaces accordingly. In this case, the following block of nodes (shortened)\n",
    "```\n",
    "node {\n",
    "  name: \"antirectifier_1/Mean/reduction_indices\"\n",
    "  op: \"Const\"\n",
    "...\n",
    "node {\n",
    "  name: \"antirectifier_1/concat\"\n",
    "  op: \"ConcatV2\"\n",
    "...\n",
    "```\n",
    "must be ALL replaced with a single node as follows:\n",
    "```\n",
    "node {\n",
    "  name: \"antirectifier_1\"\n",
    "  op: \"Antirectifier\"\n",
    "  input: \"conv2d_1/BiasAdd\"\n",
    "  attr {\n",
    "    key: \"T\"\n",
    "    value {\n",
    "      type: DT_FLOAT\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "and the node that receives the second antirectifier, which is the following\n",
    "```\n",
    "node {\n",
    "  name: \"average_pooling2d_1/AvgPool\"\n",
    "  op: \"AvgPool\"\n",
    "  input: \"antirectifier_1/concat\"\n",
    "  ...\n",
    "```\n",
    "should be changed as follows\n",
    "```\n",
    "node {\n",
    "  name: \"average_pooling2d_1/AvgPool\"\n",
    "  op: \"AvgPool\"\n",
    "  input: \"antirectifier_1\"\n",
    "  ...\n",
    "```\n",
    "Repeat the same replacement procedure for any other occurrence of the antirectifier in your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQm82RofxQo9"
   },
   "source": [
    "# Now it's up to you\n",
    "Create a new network and try to make it competitive with the previous ones, either more accurate and bigger (i.e., more layers and more parameters) or less accurate but smaller. See the slides \"Lab 1: MNIST Competition\" for more information and to know the constraints of the contest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVc03NQIPXUS"
   },
   "source": [
    "## 5. Your network\n",
    "Write your own network and give it a name. Use `AntiRectifier` as activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbJMjdgVPcSS"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "class YourNet:\n",
    "  @staticmethod\n",
    "  def build():\n",
    "    model = Sequential()\n",
    "    inputShape = (784,1)\n",
    "    model.add(Input(shape=(784,)))\n",
    "    model.add(Reshape((28, 28, 1), input_shape=(784,)))\n",
    "\n",
    "    # ...\n",
    "\n",
    "    model.add(Dense(units=10, activation = 'softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2r3WfpXQj8L"
   },
   "source": [
    "## Your training\n",
    "Remember that for the contest the number of epochs is fixed to `30` and can't be changed. All the other hyper-parameters, such as `batch_size`, `init_lr`, `opt`, etc., can be edited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhdgWwryQlxg"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "init_lr = 1e-3\n",
    "opt = Adam(lr=init_lr, decay=init_lr / (epochs * 0.5))\n",
    "#checkpoint_filepath5 = '/content/checkpoint_YourNet_{epoch:02d}_acc_{acc:.4f}_loss_{loss:.4f}_valacc_{val_acc:.4f}_valloss_{val_loss:.4f}.hdf5'\n",
    "checkpoint_filepath5 = './content/checkpoint_YourNet.hdf5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath5,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_acc', #val_accuracy in tf 2.0\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "model5 = YourNet.build()\n",
    "model5.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"acc\"])\n",
    "model5.summary()\n",
    "\n",
    "#init_op = tf.global_variables_initializer()\n",
    "\n",
    "#sess = tf.Session()\n",
    "#sess.run(init_op)\n",
    "\n",
    "# To clean the history of the training.\n",
    "H5 = []\n",
    "loss_list5 = []\n",
    "val_loss_list5 = []\n",
    "acc_list5 = []\n",
    "val_acc_list5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEMqdJIwQ_P9"
   },
   "outputs": [],
   "source": [
    "H5 = model5.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[model_checkpoint_callback], validation_split=0.1)\n",
    "\n",
    "loss_list5 += H5.history['loss']\n",
    "val_loss_list5 += H5.history['val_loss']\n",
    "acc_list5 += H5.history['acc']\n",
    "val_acc_list5 += H5.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6QJENTEceku"
   },
   "outputs": [],
   "source": [
    "# this is the score of the last epoch\n",
    "score5 = model5.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score5[0])\n",
    "print(\"Test accuracy:\", score5[1])\n",
    "\n",
    "# reload best weights\n",
    "model5.load_weights(checkpoint_filepath5)\n",
    "score5 = model5.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score5[0])\n",
    "print(\"Test accuracy:\", score5[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1rBwCHxREet"
   },
   "outputs": [],
   "source": [
    "loss_list_plot = loss_list5\n",
    "val_loss_list_plot = val_loss_list5\n",
    "acc_list_plot = acc_list5\n",
    "val_acc_list_plot = val_acc_list5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "N = np.arange(0, len(loss_list_plot))\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, loss_list_plot, label=\"train_loss\")\n",
    "plt.plot(N, val_loss_list_plot, label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim(0, 0.90) # edit this line if needed to show a better graph\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(N, acc_list_plot, label=\"train_acc\")\n",
    "plt.plot(N, val_acc_list_plot, label=\"val_acc\")\n",
    "plt.title(\"Training Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim(0.0, 1) # edit this line if needed to show a better graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKUzKZF3ML-V"
   },
   "source": [
    "## Export your model\n",
    "Repeate the previous steps for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNBrr-QNNhhd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# This line must be executed before loading Keras model.\n",
    "# If you use dropout, batch normalization or any other layers like these\n",
    "# (which have not trainable but calculating values),\n",
    "# you should change the learning phase of keras backend\n",
    "K.set_learning_phase(0) # 0 testing, 1 training mode\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model5.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1nLaX60QhTJ"
   },
   "outputs": [],
   "source": [
    "tf.train.write_graph(frozen_graph, \"model5\", \"tf_model5_MNIST.pb\", as_text=False)\n",
    "tf.train.write_graph(frozen_graph, \"model5\", \"tf_model5_MNIST.pbtxt\", as_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciy4zexpQ3NS"
   },
   "source": [
    "## Edit your graph\n",
    "Repeat the same replacement procedure for any other occurrence of the antirectifier in your network."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fsd-0nUtUR8X",
    "RVc03NQIPXUS",
    "x2r3WfpXQj8L",
    "fKUzKZF3ML-V",
    "ciy4zexpQ3NS"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
